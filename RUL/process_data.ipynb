{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mydataset import TRANSFORMER_DATA_MINDS,TRANSFORMER_ALL_DATA_MINDS\n",
    "from mymodel import mymodel, Discriminator, backboneDiscriminator\n",
    "from myloss import advLoss\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy\n",
    "import os \n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore import Model,ParameterTuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore import Model\n",
    "from mindspore import dataset as ds\n",
    "from mindspore.nn import LossBase\n",
    "from mindspore.common.initializer import Normal\n",
    "from mindspore.train.callback import LossMonitor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# class L1LossForMultiLabel(LossBase):\n",
    "#     def __init__(self, reduction=\"mean\"):\n",
    "#         super(L1LossForMultiLabel, self).__init__(reduction)\n",
    "#         self.abs = ops.Abs()\n",
    "\n",
    "#     def construct(self, base, target1, target2):\n",
    "#         x1 = self.abs(base - target1)\n",
    "#         x2 = self.abs(base - target2)\n",
    "#         return self.get_loss(x1)/2 + self.get_loss(x2)/2\n",
    "# # reformat the loss class\n",
    "        \n",
    "\n",
    "# class CustomWithLossCell(nn.Cell):\n",
    "#     def __init__(self, backbone, loss_fn):\n",
    "#         super(CustomWithLossCell, self).__init__(auto_prefix=False)\n",
    "#         self._backbone = backbone\n",
    "#         self._loss_fn = loss_fn\n",
    "\n",
    "#     def construct(self, data, label1, label2):\n",
    "#         output = self._backbone(data)\n",
    "#         return self._loss_fn(output, label1, label2)\n",
    "\n",
    "# def get_multilabel_data(num, w=2.0, b=3.0):\n",
    "#     for _ in range(num):\n",
    "#         x = np.random.uniform(-10.0, 10.0)\n",
    "#         noise1 = np.random.normal(0, 1)\n",
    "#         noise2 = np.random.normal(-1, 1)\n",
    "#         y1 = x * w + b + noise1\n",
    "#         y2 = x * w + b + noise2\n",
    "#         yield np.array([x]).astype(np.float32), np.array([y1]).astype(np.float32), np.array([y2]).astype(np.float32)\n",
    "\n",
    "# def create_multilabel_dataset(num_data, batch_size=16):\n",
    "#     dataset = ds.GeneratorDataset(list(get_multilabel_data(num_data)), column_names=['data', 'label1', 'label2'])\n",
    "#     dataset = dataset.batch(batch_size)\n",
    "#     return dataset\n",
    "\n",
    "# # initial net\n",
    "\n",
    "\n",
    "# # Set up loss\n",
    "# loss = L1LossForMultiLabel()\n",
    "# # build loss network\n",
    "# loss_net = CustomWithLossCell(net, loss)\n",
    "\n",
    "# opt = nn.Momentum(net.trainable_params(), learning_rate=0.005, momentum=0.9)\n",
    "# model = Model(network=loss_net, optimizer=opt)\n",
    "# ds_train = create_multilabel_dataset(num_data=160)\n",
    "# model.train(epoch=1, train_dataset=ds_train, callbacks=[LossMonitor()], dataset_sink_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 70\n",
    "target= 'FD002'\n",
    "source = 'FD003'\n",
    "epoches = 240\n",
    "os.chdir('/Domain-Adaptive-Remaining-Useful-Life-Prediction-with-Transformer/')\n",
    "batch_size = 1000\n",
    "a = 0.1\n",
    "b = 0.5\n",
    "\n",
    "\n",
    "def prepareData(source_list,target_list,target_test_names):\n",
    "    s_data = TRANSFORMER_ALL_DATA_MINDS(source_list, seq_len)\n",
    "    t_data = TRANSFORMER_ALL_DATA_MINDS(target_list, seq_len)\n",
    "    t_data_test = TRANSFORMER_ALL_DATA_MINDS(target_test_names, seq_len)\n",
    "    return s_data,t_data,t_data_test\n",
    "\n",
    "\n",
    "# class MultipleLoss(LossBase):\n",
    "#     def __init__(self,source, target, reduction='mean'):\n",
    "#         super(MultipleLoss, self).__init__(reduction)\n",
    "#         self.mseLoss = nn.MSELoss()\n",
    "#         self.feaLoss = advLoss(source, target)\n",
    "#         self.outLoss = advLoss(source, target)\n",
    "#         self.a = 0.1\n",
    "#         self.b = 0.5\n",
    "#         self.allLoss = 0.0\n",
    "\n",
    "#     def construct(self, s_r, s_lb, s_bkb, t_bkb, s_out, t_out):\n",
    "#         loss1 = self.mseLoss(s_r, s_lb)\n",
    "#         loss2 = self.feaLoss(s_bkb, t_bkb)\n",
    "#         loss3 = self.outLoss(s_out, t_out)\n",
    "#         return loss1 + self.a*loss2 + self.b*loss3\n",
    "    \n",
    "class MywithLossCell(nn.Cell):\n",
    "    def __init__(self,net, D1, D2,loss_fn, auto_prefix=False):\n",
    "        super(MywithLossCell, self).__init__()\n",
    "        self.net = net\n",
    "        self.D1 = D1\n",
    "        self.D2 = D2\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def construct(self,s_input, s_msk, t_input, t_msk,s_lb):\n",
    "        s_features, s_out = self.net(s_input, s_msk)\n",
    "        t_features, t_out = self.net(t_input, t_msk)\n",
    "        s_out.squeeze(2)\n",
    "        t_out.squeeze(2)\n",
    "        s_domain_bkb = self.D2(s_features)\n",
    "        t_domain_bkb = self.D2(t_features)\n",
    "        s_domain_out = self.D1(s_out)\n",
    "        t_domain_out = self.D1(t_out)\n",
    "        return self._loss_fn(s_out,s_lb\n",
    "                            ,s_domain_bkb.squeeze(1), t_domain_bkb.squeeze(1)\n",
    "                            ,s_domain_out.squeeze(1), t_domain_out.squeeze(1))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 70\n",
    "target= 'FD002'\n",
    "source = 'FD003'\n",
    "epoches = 240\n",
    "os.chdir('/Domain-Adaptive-Remaining-Useful-Life-Prediction-with-Transformer/')\n",
    "batch_size = 1000\n",
    "a = 0.1\n",
    "b = 0.5\n",
    "source_list = numpy.loadtxt(\"save/\"+source+\"/train\"+source+\".txt\", dtype=str).tolist()\n",
    "target_list = numpy.loadtxt(\"save/\"+target+\"/train\"+target+\".txt\", dtype=str).tolist()\n",
    "valid_list = numpy.loadtxt(\"save/\"+target+\"/test\"+target+\".txt\", dtype=str).tolist()\n",
    "a_list = numpy.loadtxt(\"save/\"+target+\"/valid\"+target+\".txt\", dtype=str).tolist()\n",
    "target_test_names = valid_list + a_list\n",
    "minl = min(len(source_list), len(target_list))\n",
    "s_data,t_data,t_data_test = prepareData(source_list,target_list,target_test_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_data.data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MERGED_DATA():\n",
    "    def __init__(self,s_data,t_data) -> None:\n",
    "        self.s_data = s_data\n",
    "        self.t_data = t_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(len(self.s_data),len(self.s_data))\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return self.s_data[index]+(self.t_data[index][0],self.t_data[index][2])\n",
    "        # ['s_input', 's_lb', 's_msk','t_input', 't_msk']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = ds.RandomSampler()\n",
    "all_data = MERGED_DATA(s_data,t_data)\n",
    "# t_dataset = ds.GeneratorDataset(t_data,sampler=sampler,\n",
    "#                                 column_names=['t_input', 't_nouse', 't_msk'])\n",
    "# s_dataset = ds.GeneratorDataset(s_data,sampler=sampler,\n",
    "#                                 column_names=['s_input', 's_lb', 's_msk'])\n",
    "\n",
    "dataset = ds.GeneratorDataset(all_data,sampler=sampler,column_names=['s_input', 's_lb', 's_msk','t_input', 't_msk'])\n",
    "dataset.batch(batch_size)\n",
    "class MultipleLoss(LossBase):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(MultipleLoss, self).__init__(reduction)\n",
    "        self.mseLoss = nn.MSELoss()\n",
    "        self.feaLoss = advLoss()\n",
    "        self.outLoss = advLoss()\n",
    "        self.a = 0.1\n",
    "        self.b = 0.5\n",
    "        self.allLoss = 0.0\n",
    "\n",
    "    def construct(self, s_r, s_lb, s_bkb, t_bkb, s_out, t_out):\n",
    "        loss1 = self.mseLoss(s_r, s_lb)\n",
    "        loss2 = self.feaLoss(s_bkb, t_bkb)\n",
    "        loss3 = self.outLoss(s_out, t_out)\n",
    "        return loss1 + self.a*loss2 + self.b*loss3\n",
    "\n",
    "loss_func = MultipleLoss()\n",
    "net = mymodel(max_len=seq_len,batch_size=1000)\n",
    "D1 = Discriminator(seq_len)\n",
    "D2 = backboneDiscriminator(seq_len)\n",
    "loss_net = MywithLossCell(net,D1,D2,loss_func)\n",
    "opt = nn.SGD(net.trainable_params()+D1.trainable_params()+D2.trainable_params()\n",
    "             ,learning_rate=0.02)\n",
    "model = Model(network=loss_net, optimizer=opt)\n",
    "# FORMAT two dataset into one.\n",
    "model.train(epoch=10, train_dataset=dataset, callbacks=[LossMonitor()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_data[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "303.47s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /Domain-Adaptive-Remaining-Useful-Life-Prediction-with-Transformer/RUL/process_data.ipynb to script\n",
      "[NbConvertApp] Writing 7237 bytes to /Domain-Adaptive-Remaining-Useful-Life-Prediction-with-Transformer/RUL/process_data.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script /Domain-Adaptive-Remaining-Useful-Life-Prediction-with-Transformer/RUL/process_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mindspore import nn\n",
    "from mindspore.train import Model\n",
    "\n",
    "class Net(nn.Cell):\n",
    "    def __init__(self, num_class=10, num_channel=1):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='valid')\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid')\n",
    "        self.fc1 = nn.Dense(16*5*5, 120, weight_init='ones')\n",
    "        self.fc2 = nn.Dense(120, 84, weight_init='ones')\n",
    "        self.fc3 = nn.Dense(84, num_class, weight_init='ones')\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.max_pool2d(self.relu(self.conv1(x)))\n",
    "        x = self.max_pool2d(self.relu(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits()\n",
    "optim = nn.Momentum(params=net.trainable_params(), learning_rate=0.1, momentum=0.9)\n",
    "model = Model(net, loss_fn=loss, optimizer=optim, metrics=None)\n",
    "# For details about how to build the dataset, please refer to the variable `dataset_train` in tutorial\n",
    "# document on the official website:\n",
    "# https://www.mindspore.cn/tutorials/zh-CN/r2.0.0-alpha/beginner/quick_start.html\n",
    "dataset = create_custom_dataset()\n",
    "model.train(2, dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
